# -*- coding: utf-8 -*-
"""yet_another_demo_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MgyrU_UrGyhhAzahMdWvu3Nf2TQxAT3v
"""
#!pip install datasets transformers cohere


from datasets import load_dataset, DatasetDict
from transformers import AutoTokenizer, TFXLMRobertaModel
import tensorflow as tf
import cohere
import sys
import json

token = "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"
api_key = "rFxDGAmRkjVDLBCVxsI6zuk8Ps4np6WZ8R9m3SeQ"

co = cohere.Client(api_key)

# use tpu
# tpu = tf.distribute.cluster_resolver.TPUClusterResolver("local")
# tf.config.experimental_connect_to_cluster(tpu)
# tf.tpu.experimental.initialize_tpu_system(tpu)
# strategy = tf.distribute.TPUStrategy(tpu)

model = TFXLMRobertaModel.from_pretrained("carlesoctav/en-id-parallel-sentences-mpnet-dot-v1")
tokenizer = AutoTokenizer.from_pretrained("carlesoctav/en-id-parallel-sentences-mpnet-dot-v1")

dataset = load_dataset("carlesoctav/modules-discussion-search-embedding", use_auth_token=token)

modules_docs = []
modules_embeddings = []
discussion_docs = []
discussion_embeddings = []

dataset

for doc in dataset["modulesdiscussionsearch"]:
    modules_embeddings.append(doc["embedding_vector"])
    modules_docs.append(doc)

modules_embeddings = tf.convert_to_tensor(modules_embeddings)


for doc in dataset["discussionsearch"]:
    discussion_embeddings.append(doc["embedding_vector"]) 
    discussion_docs.append(doc)

discussion_embeddings = tf.convert_to_tensor(discussion_embeddings)

modules_embeddings

#modules example
# query = "berapa banyak promise yang ada pada javascript" # indo 
# query = "apakah javascript multiparadigm?" # mix indo english

docs_for_rerank = []
docs_for_show = []

#its bilingual, so u can use english or indonesia
# query = "berapa gaji pemrogram javascript"
query =  "apa bedanya ecmascript "


response = tokenizer(query, padding="max_length", truncation=True, max_length=128, return_tensors="tf")
query_embedding = model(response).last_hidden_state[:,0,:]

top_k_param = 20 # try to change this also, if you use higher number, try to rerank with cohere
top_k_param_modules = int(0.4*top_k_param)
top_k_param_discussion = int(0.6*top_k_param)


# Compute dot score between query embedding and document embeddings
dot_scores_module = tf.matmul(query_embedding, modules_embeddings, transpose_b=True)
dot_scores_discussion = tf.matmul(query_embedding, discussion_embeddings, transpose_b=True)
top_k_modules = tf.math.top_k(dot_scores_module, k=top_k_param_modules)
top_k_discussion = tf.math.top_k(dot_scores_discussion, k=top_k_param_discussion)

 

# print("Query:", query)
for doc_id in top_k_modules.indices[0]:
    # print("Module: ", modules_docs[doc_id]["title"])
    # print("content: ", modules_docs[doc_id]["content_only_text"])
    docs_for_rerank.append(modules_docs[doc_id]["content_only_text_searchable"])
    docs_for_show.append(modules_docs[doc_id])
    
    # print("-" * 50)


for doc_id in top_k_discussion.indices[0]:
    # print("Discussion: ", discussion_docs[doc_id]["title"])
    # print("content: ", discussion_docs[doc_id]["content_only_text"])
    docs_for_rerank.append(discussion_docs[doc_id]["content_only_text_searchable"])
    docs_for_show.append(discussion_docs[doc_id])
    # print("-" * 50)

"""# better result with reranker model"""
returnJson = []
hasil = []

#fungsinya nge re-rank document yang udah di retrive dari search engine
retrive_docs_num = 5
results = co.rerank(query=query, documents=docs_for_rerank, top_n = retrive_docs_num, model = "rerank-multilingual-v2.0")
# for idx, r in enumerate(results):
    # returnJson.append(docs_for_show)
    # print(f"Document Rank: {idx + 1}, Document Index: {r.index}")
    # print(f"Document: {docs_for_show[r.index]}")
    # print(f"Relevance Score: {r.relevance_score:.2f}")
    # print("\n")
print(f"Results: {results}")



# resp = {
#     "Response":200,
#     "Message":"Data from Python ",
#     "Data" : returnJson
# }

# print(json.dumps(resp))
# sys.stdout.flush()