# -*- coding: utf-8 -*-
"""Semantic_Search.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1es2LBqIz-KBp2CGJgSECNKvcxuLbyIbv
"""


from datasets import load_dataset, DatasetDict
from transformers import AutoTokenizer, TFXLMRobertaModel
import tensorflow as tf
import cohere
import sys

token = "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"
api_key = "rFxDGAmRkjVDLBCVxsI6zuk8Ps4np6WZ8R9m3SeQ"

co = cohere.Client(api_key)

# use tpu
# tpu = tf.distribute.cluster_resolver.TPUClusterResolver("local")
# tf.config.experimental_connect_to_cluster(tpu)
# tf.tpu.experimental.initialize_tpu_system(tpu)
# strategy = tf.distribute.TPUStrategy(tpu)

from transformers import AutoTokenizer, TFAutoModel

tokenizer = AutoTokenizer.from_pretrained("carlesoctav/multi-qa-en-id-mMiniLMv2-L6-H384")

model = TFAutoModel.from_pretrained("carlesoctav/multi-qa-en-id-mMiniLMv2-L6-H384")

dataset = load_dataset("carlesoctav/modules-discussion-search-embedding", use_auth_token="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"
)

dataset

"""#Run sekali aja"""

docs1 = []
doc_embeddings1 =[]
max_docs = 100000

for doc in dataset["discussionsearch"]:
    docs1.append(doc)
    doc_embeddings1.append(doc["target_embedding"])
    if len(docs1) >= max_docs:
        break

doc_embeddings1 = tf.convert_to_tensor(doc_embeddings1)

docs = []
doc_embeddings =[]
max_docs = 100000

for doc in dataset["modulesdiscussionsearch"]:
    docs.append(doc)
    doc_embeddings.append(doc["target_embedding"])
    if len(docs) >= max_docs:
        break

doc_embeddings = tf.convert_to_tensor(doc_embeddings)

"""#Bawah sini boleh run berkali2"""

#buat searching di discussion dan module
from sorcery import dict_of

query = sys.argv[1]

response = tokenizer(query, padding="max_length", truncation=True, max_length=256, return_tensors="tf")
query_embedding = model(response).last_hidden_state[:,0,:]

top_k_param = 20 # try to change this also

# Compute dot score between query embedding and document embeddings
dot_scores_search1 = tf.matmul(query_embedding, doc_embeddings1, transpose_b=True)
top_k_modules1 = tf.math.top_k(dot_scores_search1, k=top_k_param)

for_json_discussion = []

print("Query:", query)
for doc_id in top_k_modules1.indices[0]:
  title =  docs1[doc_id]["title"]
  content_display = docs1[doc_id]["content_display"]
  document_id = docs1[doc_id]['document_id']
  data = dict_of(document_id, title, content_display)
  for_json_discussion.append(data)

dot_scores_search = tf.matmul(query_embedding, doc_embeddings, transpose_b=True)
top_k_modules = tf.math.top_k(dot_scores_search, k=top_k_param)

for_json_module = []

print("Query:", query)
for doc_id in top_k_modules.indices[0]:
  title =  docs[doc_id]["title"]
  content_display = docs[doc_id]["content_display"]
  document_id = docs[doc_id]['document_id']
  content_only_text = docs[doc_id]['content_only_text']
  data = dict_of(document_id, title, content_display, content_only_text)
  for_json_module.append(data)

# buat convert ke json
import json

with open('output_discussion.json', 'w', encoding='utf-8') as f:
    json.dump(for_json_discussion, f, ensure_ascii=False, indent=4)


with open('output_module.json', 'w', encoding='utf-8') as f:
    json.dump(for_json_module, f, ensure_ascii=False, indent=4)

